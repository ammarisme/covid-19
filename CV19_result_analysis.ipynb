{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "CV19-result-analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarisme/covid-19/blob/master/CV19_result_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKz91FjrQn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse==latest+cu101 \\\n",
        "  torch-scatter==latest+cu101 \\\n",
        "  torch-cluster==latest+cu101 \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.5.0.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYMKVm0cWK57",
        "colab_type": "code",
        "outputId": "8a44de6a-e646-400d-f6a6-d935c20aa57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "from functools import reduce\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('running on '+ (\"GPU\" if torch.cuda.is_available() else \"CPU\"))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running on CPU\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFsXKb1CzwSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/My Drive/covid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3KymLSjN91m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_file_search(mypath):\n",
        "  losses_files = [f for f in listdir(mypath) if (\"losses\" in f)]\n",
        "  ax_paths = []\n",
        "  for loss_file in losses_files:\n",
        "    losses = np.load(join(mypath, loss_file), allow_pickle=True)\n",
        "    training_loss = np.array(losses.tolist()['losses']).T[0]\n",
        "    ax = plt.plot(training_loss)\n",
        "    filepath = mypath\n",
        "    ax_paths.append((ax, filepath))\n",
        "  \n",
        "  directories = [f for f in listdir(mypath) if os.path.isdir(join(mypath, f))]\n",
        "  for directory in directories:\n",
        "    ax_paths.append(loss_file_search(mypath+'/'+directory))\n",
        "  \n",
        "  return ax_paths\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlSW4je3USF9",
        "colab_type": "code",
        "outputId": "ec72b674-78e4-41e4-da6f-c0c76aee6f77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import ylim\n",
        "\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "\n",
        "root = PATH+'/models__rnn/4'\n",
        "#ylim(top=5.08, bottom=5.07)#, ylim_bottom=5.0\n",
        "#ax_paths = loss_file_search(root)\n",
        "\"\"\"axes = [ax_path[0][0] for ax_path in ax_paths]\n",
        "paths = [ax_path[0][1][-1] for ax_path in ax_paths]\n",
        "colors = {\n",
        "    0 :\"red\",\n",
        "    3 : \"green\",\n",
        "    4 : \"blue\",\n",
        "    6: \"orange\"\n",
        "}\n",
        "plt.legend(handles=[\n",
        "                    mpatches.Patch(color=colors[int(path)], label=str(path)) for path in paths])\n",
        "plt.show()\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'axes = [ax_path[0][0] for ax_path in ax_paths]\\npaths = [ax_path[0][1][-1] for ax_path in ax_paths]\\ncolors = {\\n    0 :\"red\",\\n    3 : \"green\",\\n    4 : \"blue\",\\n    6: \"orange\"\\n}\\nplt.legend(handles=[\\n                    mpatches.Patch(color=colors[int(path)], label=str(path)) for path in paths])\\nplt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKX_tm-MVOv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CovidDataSet(InMemoryDataset):\n",
        "    def __init__(self, root, input_sequence, output_sequence, transform=None, pre_transform=None):\n",
        "        super(CovidDataSet, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "      if os.path.exists(self.root+PROCESSED_DIR):\n",
        "        return self.root+'/cleaned'\n",
        "      else:\n",
        "        os.mkdir(self.root+PROCESSED_DIR)\n",
        "        return self.root+'/cleaned'\n",
        "        \n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "      if os.path.exists(self.root+PROCESSED_DIR):\n",
        "        return self.root+PROCESSED_DIR\n",
        "      else:\n",
        "        os.mkdir(self.root+PROCESSED_DIR)\n",
        "        return self.root+PROCESSED_DIR\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "      mypath = self.raw_dir\n",
        "      filenames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "      return filenames\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['processed.dt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "    \n",
        "    def process(self):\n",
        "        \n",
        "        data_list = []\n",
        "\n",
        "        for raw_path in self.raw_paths:\n",
        "          df = pd.read_csv(raw_path)\n",
        "          for synthetic_seq in df['synthesis_seq'].unique():\n",
        "            synthetic_data = df[df['synthesis_seq']==synthetic_seq]\n",
        "\n",
        "            for country in synthetic_data['countryterritoryCode'].unique():\n",
        "              country_data = synthetic_data[synthetic_data['countryterritoryCode'] == country]\n",
        "              popData2018 = country_data['popData2018'].values[0]\n",
        "              _country_code = country_data['_country_code'].values[0]\n",
        "              \n",
        "              country_data_i = country_data[:-configuration['output_sequence_len']]\n",
        "              country_data_o = country_data[configuration['input_sequence_len']:]\n",
        "              \n",
        "              \n",
        "              country_data_array = np.array([country_data_i['cases'].to_numpy(),\n",
        "                                             country_data_i['deaths'].to_numpy()\n",
        "                                             ])\n",
        "              \"\"\"\n",
        "              country_data_i['_country_code'].to_numpy(),\n",
        "                                             country_data_i['countriesAndTerritories'].to_numpy(),\n",
        "                                             country_data_i['geoId'].to_numpy(),\n",
        "                                             country_data_i['countryterritoryCode'].to_numpy(),\n",
        "                                             country_data_i['continentExp'].to_numpy()\n",
        "              \"\"\"\n",
        "              feature_length = len(country_data_array)\n",
        "              country_data_array = country_data_array.reshape(feature_length,len(country_data_i))\n",
        "\n",
        "              country_data_array_y = np.array([country_data_o['cases'].to_numpy(), country_data_o['deaths'].to_numpy()])\n",
        "              country_data_array_y = country_data_array_y.reshape(2,len(country_data_o))\n",
        "\n",
        "              x = country_data_array[:feature_length].T\n",
        "              y = country_data_array_y[:2].T\n",
        "\n",
        "              sets =0\n",
        "              x_list = []\n",
        "              dict_x = dict()\n",
        "              for i in range(configuration['input_sequence_len']):\n",
        "                array_len = ((len(x) -i) - ((len(x)-i)%configuration['input_sequence_len']))+i\n",
        "                if array_len <= 0:\n",
        "                  continue\n",
        "                sets = int( array_len/ configuration['input_sequence_len'])\n",
        "                if sets <= 0:\n",
        "                  continue\n",
        "                #print('input seq : ', i , ' ', array_len , ' ',array_len-i , ' number of sets : ', sets)\n",
        "                x_temp = x[i:array_len].T.reshape(sets,feature_length,configuration['input_sequence_len'])\n",
        "                x_temp = x_temp.reshape(feature_length,sets,configuration['input_sequence_len'])\n",
        "                uniq_keys = np.array([i+(configuration['input_sequence_len']*k) for k in range(configuration['input_sequence_len'])])\n",
        "                \n",
        "                arrays_split = np.hsplit(x_temp,sets)\n",
        "                dict_x.update(dict(zip(uniq_keys, arrays_split)))\n",
        "              \n",
        "              dict_y = dict()\n",
        "              y_list = []\n",
        "              for i in range(configuration['output_sequence_len']):\n",
        "                array_len_y = (len(y)-i) - ((len(y)  - i)%configuration['output_sequence_len'])+i\n",
        "                if array_len_y <= 0:\n",
        "                  continue\n",
        "                sets = int(array_len_y / configuration['output_sequence_len'])\n",
        "                if sets <= 0:\n",
        "                  continue\n",
        "                \n",
        "                #print('output seq : ', i , ' ', array_len_y , ' ',array_len_y-(i) , ' number of sets : ', sets)\n",
        "                y_temp = y[i:array_len_y].T.reshape(sets, 2, configuration['output_sequence_len'])\n",
        "                uniq_keys = np.array([i+(configuration['output_sequence_len']*k) for k in range(configuration['output_sequence_len'])])\n",
        "                y_temp = y_temp.reshape(2,sets,configuration['output_sequence_len'])\n",
        "                arrays_split = np.hsplit(y_temp,sets)\n",
        "                dict_y.update(dict(zip(uniq_keys, arrays_split)))\n",
        "              \n",
        "\n",
        "              temp_x_list  = [dict_x[i].T for i in sorted(dict_x.keys())]\n",
        "              temp_y_list  = [dict_y[i].T for i in sorted(dict_y.keys())]\n",
        "\n",
        "              #_country_code,popData2018\n",
        "              xy_list = [Data(x = torch.from_numpy(features).type(torch.FloatTensor).squeeze()) for features in temp_x_list]\n",
        "\n",
        "              for i in sorted(dict_y.keys()):\n",
        "                xy_list[i].y = torch.from_numpy(temp_y_list[i]).squeeze()\n",
        "\n",
        "              data_list += xy_list\n",
        "          print('processed : '+ raw_path)\n",
        "        data, slices = self.collate(data_list)\n",
        "        torch.save((data, slices), self.processed_paths[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkZyInZqYdRa",
        "colab_type": "code",
        "outputId": "d3e2aba2-8ad6-42bc-9bd4-2a6851db0b01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "configuration = {\n",
        "    'input_sequence_len' : 10,\n",
        "    'output_sequence_len' : 10,\n",
        "    'training_batch_size' : 1024,\n",
        "    'training_dataset_length' :32768,\n",
        "    'validation_batch_size' : 1024,\n",
        "    'yhat_size' : 2,\n",
        "    'feature_len' : 2,\n",
        "    'output_size' : 2,\n",
        "}\n",
        "\n",
        "\n",
        "INPUT_ROOT = PATH+'/input_test'\n",
        "DATA_TAG = \"seq2seq_\"+str(configuration['input_sequence_len'])+'_'+str(configuration['output_sequence_len'])\n",
        "PROCESSED_DIR = '/processed_'+DATA_TAG\n",
        "\n",
        "validation_dataset = CovidDataSet(INPUT_ROOT, configuration['input_sequence_len'], configuration['output_sequence_len'])\n",
        "validation_dataset = validation_dataset.shuffle()\n",
        "validation_dataset = validation_dataset[3000:]\n",
        "validation_dataloader = DataLoader(validation_dataset,batch_size=configuration['validation_batch_size'])\n",
        "\n",
        "print('batches validation :', len(validation_dataloader))\n",
        "print('dataset length validation :', len(validation_dataset))\n",
        "print('sample data :', validation_dataset[100].x)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing...\n",
            "processed : /content/drive/My Drive/covid/input_test/cleaned/Copy of cleaned_worldwide_cases_27_04.csv\n",
            "Done!\n",
            "batches validation : 7\n",
            "dataset length validation : 6260\n",
            "sample data : tensor([[ 0.,  0.],\n",
            "        [ 0.,  0.],\n",
            "        [18.,  0.],\n",
            "        [ 0.,  0.],\n",
            "        [ 9.,  0.],\n",
            "        [ 2.,  0.],\n",
            "        [10.,  0.],\n",
            "        [ 0.,  0.],\n",
            "        [ 1.,  0.],\n",
            "        [ 4.,  0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUrnl4KvhkWN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, parameter_sizes, repeats ,output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.repeater_input_size = parameter_sizes[0]\n",
        "        self.hidden_size = parameter_sizes[1]\n",
        "        self.repeats = repeats\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, self.repeater_input_size, bias=False)\n",
        "        self.relu_activation = nn.ReLU()\n",
        "\n",
        "        self.layers = dict()\n",
        "\n",
        "        \n",
        "        k = 0\n",
        "        for i in range(repeats):\n",
        "          i = i+k\n",
        "          self.layers['fc_'+str(i)] = nn.Linear(self.repeater_input_size, self.hidden_size)\n",
        "          self.layers['gru_'+str(i+1)] = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "          self.layers['fc_'+str(i+2)] = nn.Linear(self.hidden_size, self.repeater_input_size)\n",
        "          k+=2\n",
        "\n",
        "        self.module_list = nn.ModuleDict(self.layers)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.repeater_input_size, output_size)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "      output = self.fc1(input)\n",
        "      output = self.relu_activation(output)\n",
        "      print('input :',input)\n",
        "      print('output :', output.sum())\n",
        "      k = 0\n",
        "      for i in range(self.repeats):\n",
        "        i = i+k\n",
        "        output = self.layers['fc_'+str(i)](output)\n",
        "        output = self.relu_activation(output)\n",
        "\n",
        "        output, hidden[i-k] = self.layers['gru_'+str(i+1)](output, hidden[i-k])#should be different. check the nlp page\n",
        "        output = self.relu_activation(output)\n",
        "        hidden[i-k] = self.relu_activation(hidden[i-k])\n",
        "\n",
        "        output = self.layers['fc_'+str(i+2)](output)\n",
        "        output = self.relu_activation(output)\n",
        "        k +=2\n",
        "\n",
        "      output = self.fc2(output)\n",
        "      output = self.relu_activation(output)\n",
        "\n",
        "      return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Je__Iq5Khp9P",
        "colab_type": "code",
        "outputId": "cbf4df91-5be7-4cc1-b7eb-dcde1a0a079b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        }
      },
      "source": [
        "def predict(input_tensor, rnn_model):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = input_tensor.type(torch.FloatTensor)\n",
        "\n",
        "    rnn_model_hidden = []\n",
        "    for i in range(5):\n",
        "      rnn_model_hidden.append(rnn_model.initHidden(1))\n",
        "\n",
        "    validation_loss = 0\n",
        "    \n",
        "    outputs = []\n",
        "\n",
        "    for ei in range(configuration['input_sequence_len']):\n",
        "      input_tensor_seq = input_tensor.view(configuration['input_sequence_len'],1, configuration['feature_len'])[ei]\n",
        "      \n",
        "      input_tensor_seq = input_tensor_seq.view(1, 1, configuration['feature_len']).to(device)\n",
        "      rnn_model_output, rnn_model_hidden = rnn_model(\n",
        "          input_tensor_seq, rnn_model_hidden)\n",
        "      outputs.append(rnn_model_output.detach().numpy())\n",
        "\n",
        "  return outputs\n",
        "model = RNNModel(**{\n",
        "         'input_size': 2,\n",
        "         'parameter_sizes': [256, 128] ,\n",
        "         'repeats' : 5 ,\n",
        "         'output_size': 2\n",
        "         })\n",
        "\n",
        "model.eval()\n",
        "out1 = np.array(predict(torch.tensor(validation_dataset[50].x), model)).squeeze()\n",
        "out2 = np.array(predict(torch.tensor(validation_dataset[100].x), model)).squeeze()\n",
        "if np.sum(out1 - out2)==0: print('Model is broken')"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[9., 0.]]])\n",
            "output : tensor(459.1591)\n",
            "input : tensor([[[2., 0.]]])\n",
            "output : tensor(102.0354)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[3., 0.]]])\n",
            "output : tensor(153.0531)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[18.,  0.]]])\n",
            "output : tensor(918.3182)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[9., 0.]]])\n",
            "output : tensor(459.1591)\n",
            "input : tensor([[[2., 0.]]])\n",
            "output : tensor(102.0354)\n",
            "input : tensor([[[10.,  0.]]])\n",
            "output : tensor(510.1768)\n",
            "input : tensor([[[0., 0.]]])\n",
            "output : tensor(0.)\n",
            "input : tensor([[[1., 0.]]])\n",
            "output : tensor(51.0177)\n",
            "input : tensor([[[4., 0.]]])\n",
            "output : tensor(204.0707)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNmtkfNkk5xx",
        "colab_type": "code",
        "outputId": "94f8e4bc-fa73-465f-eb56-9ec3bfc4af44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "print('weights',np.mean(model.fc1.weight.detach().numpy()))\n",
        "print('bias',np.mean(model.fc1.bias.detach().numpy()))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weights 0.016845142\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-affcffa869f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'detach'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy536wg3Yxgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = RNNModel(**{\n",
        "         'input_size': 2,\n",
        "         'parameter_sizes': [256, 128] ,\n",
        "         'repeats' : 5 ,\n",
        "         'output_size': 2\n",
        "         })\n",
        "#model_statedict = torch.load(root+'/model_manual_save_0628_5.pt', map_location=torch.device('cpu'))\n",
        "#model.load_state_dict(model_statedict)\n",
        "#model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXvYVErxejcj",
        "colab_type": "code",
        "outputId": "b8c56ac7-a50c-4825-a045-c6a7d3a358b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "out1 = np.array(predict(torch.tensor(validation_dataset[50].x), model)).squeeze()\n",
        "out2 = np.array(predict(torch.tensor(validation_dataset[100].x), model)).squeeze()\n",
        "if np.sum(out1 - out2)==0: print('Model is broken')"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-33fdf52dffbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mout2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mout2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model is broken'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-bdb7417e6a32>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(input_tensor, rnn_model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_sequence_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m       \u001b[0minput_tensor_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_sequence_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0minput_tensor_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_tensor_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[10, 1, 2]' is invalid for input of size 70"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83UftElCxi27",
        "colab_type": "code",
        "outputId": "c2a641ea-3718-411a-8d5e-d7f65cfaab55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "def get_all_directories(mypath):  \n",
        "  directories= [f for f in listdir('/'+mypath) if os.path.isdir(join(mypath, f))]\n",
        "  for directory in directories:\n",
        "    get_all_directories(directory)\n",
        "    #loss_file_search(mypath+'/'+directory)\n",
        "\n",
        "get_all_directories(PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-135-8efde3e24b20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#loss_file_search(mypath+'/'+directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mget_all_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-135-8efde3e24b20>\u001b[0m in \u001b[0;36mget_all_directories\u001b[0;34m(mypath)\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mdirectories\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mget_all_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m#loss_file_search(mypath+'/'+directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-135-8efde3e24b20>\u001b[0m in \u001b[0;36mget_all_directories\u001b[0;34m(mypath)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mdirectories\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmypath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirectories\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_all_directories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#loss_file_search(mypath+'/'+directory)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/input'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHfsG7LGx5bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtNJT5sQRI7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_analysis(losses):\n",
        " print('loss analysis') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-bCsXyXOOdg",
        "colab_type": "code",
        "outputId": "56b07f0d-aa61-4ba8-f0a3-07a240f8d4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CV19 Cleaning.ipynb',\n",
              " 'CV19_Synthesis.ipynb',\n",
              " '_Common.ipynb',\n",
              " 'common (1).py',\n",
              " 'common.py',\n",
              " 'CV19Net.ipynb',\n",
              " 'rnn_model_configurations_1001_443838.csv',\n",
              " 'rnn_model_configurations_1001_165567.csv',\n",
              " 'rnn_model_configurations_1001_353166.csv',\n",
              " 'rnn_model_configurations_1001_57460.csv',\n",
              " 'rnn_model_configurations_1001_981028.csv',\n",
              " 'CV19 Dataset Explore.ipynb',\n",
              " 'CV19_Dataset.ipynb',\n",
              " 'RNN-CV19Net.ipynb',\n",
              " 'CV19-result-analysis.ipynb']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wq7W28W7Pv4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}