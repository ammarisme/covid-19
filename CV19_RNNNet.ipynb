{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "RNN-CV19Net.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ammarisme/covid-19/blob/master/CV19_RNNNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C83XgaWprQnn",
        "colab_type": "code",
        "outputId": "cbad0529-e5de-4d56-c353-d59d207846db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install torch-geometric \\\n",
        "  torch-sparse==latest+cu101 \\\n",
        "  torch-scatter==latest+cu101 \\\n",
        "  torch-cluster==latest+cu101 \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.5.0.html"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.5.0.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.6/dist-packages (1.4.3)\n",
            "Collecting torch-sparse==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_sparse-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-scatter==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_scatter-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Collecting torch-cluster==latest+cu101\n",
            "  Using cached https://pytorch-geometric.com/whl/torch-1.5.0/torch_cluster-latest%2Bcu101-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.18.3)\n",
            "Requirement already satisfied: plyfile in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.7.2)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.4)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (5.0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.22.2.post1)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.48.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.5.0+cu101)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (0.16.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (2.10.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torch-geometric) (1.0.3)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (2.4.7)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (0.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from rdflib->torch-geometric) (1.12.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->torch-geometric) (0.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (46.1.3)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba->torch-geometric) (0.31.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->torch-geometric) (4.4.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torch-geometric) (0.16.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (7.0.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (3.2.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image->torch-geometric) (1.1.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torch-geometric) (2.9)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torch-geometric) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->torch-geometric) (0.10.0)\n",
            "Installing collected packages: torch-sparse, torch-scatter, torch-cluster\n",
            "  Found existing installation: torch-sparse 0.6.2\n",
            "    Uninstalling torch-sparse-0.6.2:\n",
            "      Successfully uninstalled torch-sparse-0.6.2\n",
            "  Found existing installation: torch-scatter 2.0.4\n",
            "    Uninstalling torch-scatter-2.0.4:\n",
            "      Successfully uninstalled torch-scatter-2.0.4\n",
            "  Found existing installation: torch-cluster 1.5.4\n",
            "    Uninstalling torch-cluster-1.5.4:\n",
            "      Successfully uninstalled torch-cluster-1.5.4\n",
            "Successfully installed torch-cluster-1.5.4 torch-scatter-2.0.4 torch-sparse-0.6.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch_cluster",
                  "torch_scatter",
                  "torch_sparse"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRUdaEpW2GsF",
        "colab_type": "text"
      },
      "source": [
        "!pip install torch-geometric \\\n",
        "  torch-sparse==latest+cu101 \\\n",
        "  torch-scatter==latest+cu101 \\\n",
        "  torch-cluster==latest+cu101 \\\n",
        "  -f https://pytorch-geometric.com/whl/torch-1.5.0.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anKz91FjrQn3",
        "colab_type": "code",
        "outputId": "2a5498db-8b11-4a3d-8e1c-ddf176ab612b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import os\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader, InMemoryDataset\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('running on '+ str(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running on cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFsXKb1CzwSY",
        "colab_type": "code",
        "outputId": "19ad5a1f-4ac7-473d-f6e6-d239ef476574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import os.path\n",
        "from os import path\n",
        "import sys\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/My Drive/covid'\n",
        "sys.path.append(PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HidtRwtG9CFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CovidDataSet(InMemoryDataset):\n",
        "    def __init__(self, root, input_sequence, output_sequence, transform=None, pre_transform=None):\n",
        "        super(CovidDataSet, self).__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self):\n",
        "      if os.path.exists(self.root+'/cleaned'):\n",
        "        return self.root+'/cleaned'\n",
        "      else:\n",
        "        os.mkdir(self.root+'/cleaned')\n",
        "        return self.root+'/cleaned'\n",
        "        \n",
        "    @property\n",
        "    def processed_dir(self):\n",
        "      if os.path.exists(self.root+PROCESSED_DIR):\n",
        "        return self.root+PROCESSED_DIR\n",
        "      else:\n",
        "        os.mkdir(self.root+PROCESSED_DIR)\n",
        "        return self.root+PROCESSED_DIR\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "      mypath = self.raw_dir\n",
        "      filenames = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "      return filenames\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return ['processed.dt']\n",
        "\n",
        "    def download(self):\n",
        "        pass\n",
        "    \n",
        "    def process(self):\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rv5DjHyrQor",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, parameter_sizes, repeats ,output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.repeater_input_size = parameter_sizes[0]\n",
        "        self.hidden_size = parameter_sizes[1]\n",
        "        self.repeats = repeats\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, self.repeater_input_size)\n",
        "        self.relu_activation = nn.ReLU()\n",
        "\n",
        "        self.layers = dict()\n",
        "\n",
        "        \n",
        "        k = 0\n",
        "        for i in range(repeats):\n",
        "          i = i+k\n",
        "          self.layers['fc_'+str(i)] = nn.Linear(self.repeater_input_size, self.hidden_size)\n",
        "          self.layers['gru_'+str(i+1)] = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "          self.layers['fc_'+str(i+2)] = nn.Linear(self.hidden_size, self.repeater_input_size)\n",
        "          k+=2\n",
        "\n",
        "        self.module_list = nn.ModuleDict(self.layers)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.repeater_input_size, output_size)\n",
        "        \n",
        "    def forward(self, input, hidden):\n",
        "      output = self.fc1(input)\n",
        "      output = self.relu_activation(output)\n",
        "\n",
        "      k = 0\n",
        "      for i in range(self.repeats):\n",
        "        i = i+k\n",
        "        output = self.layers['fc_'+str(i)](output)\n",
        "        output = self.relu_activation(output)\n",
        "\n",
        "        output, hidden[i-k] = self.layers['gru_'+str(i+1)](output, hidden[i-k])#should be different. check the nlp page\n",
        "        output = self.relu_activation(output)\n",
        "        hidden[i-k] = self.relu_activation(hidden[i-k])\n",
        "\n",
        "        output = self.layers['fc_'+str(i+2)](output)\n",
        "        output = self.relu_activation(output)\n",
        "        k +=2\n",
        "\n",
        "      output = self.fc2(output)\n",
        "      output = self.relu_activation(output)\n",
        "\n",
        "      return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        return torch.zeros(1, batch_size, self.hidden_size, device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZBmfYEtiWem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_rnn(input_tensor, target_tensor, rnn_model, rnn_model_optimizer, criterion,batch_size):\n",
        "  input_tensor = input_tensor.type(torch.FloatTensor)\n",
        "  target_tensor= target_tensor.type(torch.FloatTensor)\n",
        "\n",
        "  rnn_model_hidden = []\n",
        "  for i in range(rnn_model.repeats):\n",
        "    rnn_model_hidden.append(rnn_model.initHidden(batch_size))\n",
        "\n",
        "  rnn_model_optimizer.zero_grad()\n",
        "  \n",
        "  loss = 0  \n",
        "  for ei in range(input_sequence_len):\n",
        "    target_tensor_seq = target_tensor.view(output_sequence_len, batch_size, yhat_size)\n",
        "    input_tensor_seq = input_tensor.view(input_sequence_len,batch_size, feature_len)[ei]\n",
        "    input_tensor_seq = input_tensor_seq.view(1, batch_size, feature_len).to(device)\n",
        "\n",
        "    rnn_model_output, rnn_model_hidden = rnn_model(\n",
        "        input_tensor_seq, rnn_model_hidden)\n",
        "    #print(rnn_model_output.size(), target_tensor_seq[ei].size())\n",
        "    loss += criterion(rnn_model_output.squeeze(), target_tensor_seq[ei].squeeze().to(device))\n",
        "\n",
        "  loss.backward()\n",
        "  rnn_model_optimizer.step()\n",
        "\n",
        "  return loss.item() / input_sequence_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Bb9yefOKZrz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_rnn(input_tensor, target_tensor, rnn_model, criterion,batch_size):\n",
        "  with torch.no_grad():\n",
        "    input_tensor = input_tensor.type(torch.FloatTensor)\n",
        "    target_tensor= target_tensor.type(torch.FloatTensor)\n",
        "\n",
        "    rnn_model_hidden = []\n",
        "    for i in range(rnn_model.repeats):\n",
        "      rnn_model_hidden.append(rnn_model.initHidden(batch_size))\n",
        "\n",
        "    validation_loss = 0\n",
        "    \n",
        "    for ei in range(input_sequence_len):\n",
        "      target_tensor_seq = target_tensor.view(output_sequence_len, batch_size, yhat_size)\n",
        "      input_tensor_seq = input_tensor.view(input_sequence_len,batch_size, feature_len)[ei]\n",
        "      input_tensor_seq = input_tensor_seq.view(1, batch_size, feature_len).to(device)\n",
        "      #target_tensor_seq = target_tensor.view(1, batch_size, feature_len)\n",
        "\n",
        "      rnn_model_output, rnn_model_hidden = rnn_model(\n",
        "          input_tensor_seq, rnn_model_hidden)\n",
        "      validation_loss += criterion(rnn_model_output.squeeze(), target_tensor_seq[ei].squeeze().to(device))\n",
        "\n",
        "  return validation_loss.item() / input_sequence_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3sGikLKufvN",
        "colab_type": "code",
        "outputId": "12a74134-1e58-465b-f27d-71e895436e26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "input_sequence_len = 5\n",
        "output_sequence_len = 5\n",
        "training_batch_size = 8192 # for production 131072\n",
        "validation_batch_size = 2048\n",
        "yhat_size = 2\n",
        "feature_len = 8\n",
        "feature_length = 8\n",
        "output_size = 2\n",
        "\n",
        "INPUT_ROOT = PATH+'/input'\n",
        "DATA_TAG = \"seq2seq_5_5\"\n",
        "PROCESSED_DIR = '/processed_'+DATA_TAG\n",
        "\n",
        "training_dataset = CovidDataSet(INPUT_ROOT+'/training', input_sequence_len, input_sequence_len)\n",
        "training_dataset = training_dataset.shuffle()\n",
        "training_dataset = training_dataset[:32768]\n",
        "training_dataloader = DataLoader(training_dataset,batch_size=training_batch_size)\n",
        "\n",
        "validation_dataset = CovidDataSet(INPUT_ROOT+'/validation', input_sequence_len, output_sequence_len)\n",
        "validation_dataset = validation_dataset.shuffle()\n",
        "validation_dataloader = DataLoader(validation_dataset,batch_size=validation_batch_size)\n",
        "\n",
        "print('batches validation :', len(validation_dataloader), 'training : ', len(training_dataloader))\n",
        "print('dataset length validation :', len(validation_dataset), 'training : ', len(training_dataset))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batches validation : 3 training :  4\n",
            "dataset length validation : 4943 training :  32768\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHLM8BnLR9sv",
        "colab_type": "code",
        "outputId": "fb61e83d-9903-439e-e5a2-ba67da5a453e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def generate_repeat_counts(alphas , training_dataset_length, feature_len,yhat_size,\\\n",
        "                           enc_repeated_nn_parameters = 132096):\n",
        "  repeats = []\n",
        "  model1 = RNNModel(**{\n",
        "         'input_size': feature_len,\n",
        "         'parameter_sizes': [256, 256] ,\n",
        "         'repeats' : 1 ,\n",
        "         'output_size' : 2\n",
        "        })\n",
        "  model2 = RNNModel(**{\n",
        "         'input_size': feature_len,\n",
        "         'parameter_sizes': [256, 256] ,\n",
        "         'repeats' : 2 ,\n",
        "         'output_size' : 2\n",
        "        }\n",
        "  )\n",
        "\n",
        "  enc_repeated_nn_parameters = get_n_params(model2) - get_n_params(model1)\n",
        "\n",
        "  for alpha in alphas:\n",
        "    preferred_parameters  = (1/alpha)* (training_dataset_length / (feature_len+yhat_size))\n",
        "    enc_repeats = int(\n",
        "        (preferred_parameters - \n",
        "        (preferred_parameters%enc_repeated_nn_parameters))\n",
        "        / enc_repeated_nn_parameters)\n",
        "    for j in range(1, enc_repeats+1):\n",
        "      if j not in repeats:\n",
        "        repeats.append(j)\n",
        "  return repeats\n",
        "\n",
        "def generate_model_parameters(repeat_counts, feature_len, output_len):\n",
        "  model_dict = dict()\n",
        "  model_id = 0\n",
        "  for enc_repeat in repeat_counts:\n",
        "    model_dict[model_id] = [{\n",
        "         'input_size': feature_len,\n",
        "         'parameter_sizes': [128, 128] ,\n",
        "         'repeats' : enc_repeat ,\n",
        "         'output_size' : 2\n",
        "        }]\n",
        "    model_id+=1\n",
        "  return model_dict\n",
        "\n",
        "def save_model_parameters(model_parameters_dict, models = None, fileName=None):\n",
        "  if fileName == None:\n",
        "    fileName = 'model_parameters_'+str(np.round(np.random.randn(1000,200)[0][0]*1000))\n",
        "\n",
        "  model_summary = pd.DataFrame(columns =[])\n",
        "  for i in sorted(model_parameters_dict.keys()):\n",
        "    param_size = 0\n",
        "    if models != None:\n",
        "      param_size = get_n_params(models[i][0])\n",
        "      param_size += get_n_params(models[i][1])\n",
        "    row = pd.Series({\n",
        "                        'model_id':i,\n",
        "                        'repeats' : model_parameters_dict[i][0]['repeats'],\n",
        "                        'parameters' :param_size\n",
        "    })\n",
        "    row_df = pd.DataFrame([row], index = [i])\n",
        "    model_summary = pd.concat([model_summary, row_df])\n",
        "  model_summary.to_csv(fileName)\n",
        "\n",
        "MODEL_PREFIX = '_rnn'\n",
        "MODEL_DIR =  PATH + '/models_'+MODEL_PREFIX\n",
        "\n",
        "if os.path.exists(MODEL_DIR)==False:\n",
        "  os.mkdir(MODEL_DIR)\n",
        "\n",
        "session_id =str(1001)+ '_'+str(datetime.now(tz=None).microsecond)\n",
        "repeat_counts = generate_repeat_counts([1,2,3], 500*len(training_dataset), feature_len = 8, yhat_size=2)\n",
        "\n",
        "model_configurations = generate_model_parameters(repeat_counts, 8, 2)\n",
        "save_model_parameters(model_configurations, fileName= MODEL_DIR + '/rnn_model_configurations_'+session_id+'.csv')\n",
        "models = dict()\n",
        "print(len(model_configurations))\n",
        "print(len(repeat_counts))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W34bK4bcqCMi",
        "colab_type": "code",
        "outputId": "6f4042d7-263b-446a-9fd1-ebebad00f827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "#Run all the candidate models\n",
        "\n",
        "learning_rate = 0.1\n",
        "for i in model_configurations:\n",
        "  #instantiate the models\n",
        "  if i < 0:\n",
        "    print('skipping', i)\n",
        "    continue\n",
        "  current_model_dir =  MODEL_DIR +'/'+str(i)\n",
        "  if os.path.exists(current_model_dir) == False:\n",
        "    os.mkdir(current_model_dir)\n",
        "  \n",
        "  rnn_model = RNNModel(**model_configurations[i][0]).to(device)\n",
        "  rnn_model_optimizer= optim.Adam(rnn_model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  losses = trainValidationIters(training_dataloader,validation_dataloader,\n",
        "                    rnn_model,\n",
        "                    rnn_model_optimizer,\n",
        "                    criterion, n_iters = 300,output_sequence_len=output_sequence_len,\n",
        "                    input_sequence_len=input_sequence_len,\n",
        "                    current_model_dir=current_model_dir,  model_id = i,session_id = session_id,\n",
        "                     print_every=150, save_every = 10)\n",
        "  save_dict({\n",
        "      'model' : rnn_model.state_dict(),\n",
        "      'rnn_model_optimizer' : rnn_model_optimizer.state_dict(),\n",
        "      'losses' : losses\n",
        "  }, current_model_dir+'/'+str(i)+'models_'+session_id+'.pt')\n",
        "\n",
        "  print('Model ',str(i), ': trained and evaluated')\n",
        "  \n",
        "  models[i] = {\n",
        "      'rnn_model' : rnn_model.state_dict(),\n",
        "      'rnn_model_optimizer' : rnn_model_optimizer.state_dict(),\n",
        "      'losses' : losses\n",
        "  }\n",
        "\n",
        "save_dict(models, MODEL_DIR+'/models_'+session_id+'.pt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-22ce018bc75d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                     \u001b[0minput_sequence_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_sequence_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                     \u001b[0mcurrent_model_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurrent_model_dir\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mmodel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                      print_every=150, save_every = 10)\n\u001b[0m\u001b[1;32m     23\u001b[0m   save_dict({\n\u001b[1;32m     24\u001b[0m       \u001b[0;34m'model'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-b3cca07923df>\u001b[0m in \u001b[0;36mtrainValidationIters\u001b[0;34m(train_dataloader, validation_dataloader, rnn_model, rnn_model_optimizer, criterion, n_iters, output_sequence_len, input_sequence_len, current_model_dir, model_id, session_id, print_every, plot_every, save_every)\u001b[0m\n\u001b[1;32m     36\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iters\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m       training_loss = train_batch(train_dataloader, rnn_model,\n\u001b[0;32m---> 38\u001b[0;31m                                   rnn_model_optimizer, criterion )\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       validation_loss = batch_evaluate(validation_dataloader, rnn_model,\n",
            "\u001b[0;32m<ipython-input-20-b3cca07923df>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(dataloader, rnn_model, rnn_model_optimizer, criterion)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_model_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0minput_sequence_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     loss += (train_rnn(batch.x.view(batch_size, input_sequence_len , feature_len), batch.y, rnn_model,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    186\u001b[0m         dataset at the specified indices.\"\"\"\n\u001b[1;32m    187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/in_memory_dataset.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 s[self.data.__cat_dim__(key,\n\u001b[1;32m     78\u001b[0m                                         item)] = slice(slices[idx],\n\u001b[0;32m---> 79\u001b[0;31m                                                        slices[idx + 1])\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch_geometric/data/data.py\u001b[0m in \u001b[0;36m__cat_dim__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# `*index*` and `*face*` should be concatenated in the last dimension,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;31m# everything else in the first dimension.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'(index|face)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__inc__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \"\"\"Scan through string looking for a match to the pattern, returning\n\u001b[1;32m    181\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# internal: compile pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetlocale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLC_CTYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk5WWsBe43Jz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_batch(dataloader, rnn_model, rnn_model_optimizer, criterion):\n",
        "  loss = 0\n",
        "  for batch in dataloader:\n",
        "    batch_size = int(len(batch.x)/input_sequence_len)\n",
        "    loss += (train_rnn(batch.x.view(batch_size, input_sequence_len , feature_len), batch.y, rnn_model,\n",
        "                      rnn_model_optimizer, criterion, batch_size))/ batch_size\n",
        "  return loss / len(dataloader)\n",
        "\n",
        "def batch_evaluate(dataloader, rnn_model, criterion, output_sequence_len):\n",
        "  loss = 0\n",
        "  for batch in dataloader:\n",
        "    batch_size = int(len(batch.x)/input_sequence_len)\n",
        "    loss += (evaluate_rnn(batch.x.view(batch_size, input_sequence_len , feature_len), batch.y, rnn_model,\n",
        "                     criterion, batch_size))/ batch_size\n",
        "  return loss / len(dataloader)\n",
        "\n",
        "def trainValidationIters(train_dataloader,validation_dataloader, rnn_model, rnn_model_optimizer, \n",
        "                        criterion ,n_iters, output_sequence_len,input_sequence_len,\n",
        "                        current_model_dir, model_id,session_id,print_every=1000, plot_every=100,save_every = 100):\n",
        "  #yhat_size = 2\n",
        "  #input_sequence_len = 10\n",
        "\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  \n",
        "  # Reset every print_every # Reset every plot_every\n",
        "  print_training_loss_total = 0\n",
        "  plot_training_loss_total = 0\n",
        "  print_validation_loss_total = 0\n",
        "  plot_validation_loss_total = 0\n",
        "  save_validation_loss_total = 0\n",
        "  save_training_loss_total = 0\n",
        "      \n",
        "  losses = []\n",
        "\n",
        "  for iter in range(1, n_iters + 1):\n",
        "      training_loss = train_batch(train_dataloader, rnn_model,\n",
        "                                  rnn_model_optimizer, criterion )\n",
        "\n",
        "      validation_loss = batch_evaluate(validation_dataloader, rnn_model,\n",
        "                                       criterion,output_sequence_len )\n",
        "      \n",
        "      print_training_loss_total += training_loss\n",
        "      plot_training_loss_total += training_loss\n",
        "      print_validation_loss_total += validation_loss\n",
        "      plot_validation_loss_total += validation_loss\n",
        "      \n",
        "      save_training_loss_total += training_loss\n",
        "      save_validation_loss_total += validation_loss\n",
        "      \n",
        "      if iter % print_every == 0:\n",
        "          print_training_loss_avg = print_training_loss_total / print_every\n",
        "          print_training_loss_total = 0\n",
        "          print_validation_loss_avg = print_validation_loss_total / print_every\n",
        "          print_validation_loss_total = 0\n",
        "          print('%s (%d %d%%) training:%.4f validation:%.4f' % (timeSince(start, iter / n_iters),\n",
        "                                        iter, iter / n_iters * 100, print_training_loss_avg,print_validation_loss_avg, ))\n",
        "          print('train/validation difference : ',print_validation_loss_avg-print_training_loss_avg)\n",
        "\n",
        "      #if iter % plot_every == 0:\n",
        "      #    plot_training_loss_avg = plot_training_loss_total / plot_every\n",
        "      #    plot_validation_loss_avg = plot_validation_loss_total / plot_every\n",
        "      #    plot_training_loss.append(plot_training_loss_avg)\n",
        "      #    plot_validation_loss.append(plot_validation_loss_avg)\n",
        "      #    showPlot(plot_training_losses,plot_validation_loss)\n",
        "      #    plot_training_loss_total = 0\n",
        "      #    plot_validation_loss_total\n",
        "        \n",
        "      if iter % save_every == 0:\n",
        "        save_training_loss_avg = save_training_loss_total / save_every\n",
        "        save_validation_loss_avg = plot_validation_loss_total / save_every\n",
        "        \n",
        "        #'arch': args.arch,\n",
        "        save_checkpoint({\n",
        "            'epoch': iter + 1,\n",
        "            'rnn_model_state_dict': rnn_model.state_dict(),\n",
        "            'training_loss': save_training_loss_avg,\n",
        "            'validation_loss': save_validation_loss_avg,\n",
        "            'rnn_model_optimizer' : rnn_model_optimizer.state_dict(),\n",
        "        }, False, filename=current_model_dir+'/'+str(iter)+'_'+str(session_id)+'.pt')\n",
        "      losses.append([training_loss, validation_loss])\n",
        "\n",
        "  save_dict({\n",
        "      'losses' : losses\n",
        "  }, filename=current_model_dir+'/losses_'+session_id)\n",
        "\n",
        "  return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbZSFpPGX215",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Common function\n",
        "\"\"\"\n",
        "def get_n_params(model):\n",
        "    pp=0\n",
        "    for p in list(model.parameters()):\n",
        "        nn=1\n",
        "        for s in list(p.size()):\n",
        "            nn = nn*s\n",
        "        pp += nn\n",
        "    return pp\n",
        "  \n",
        "\n",
        "def save_model_parameters(model_parameters_dict, models = None, fileName=None):\n",
        "  if fileName == None:\n",
        "    fileName = 'model_parameters_'+str(np.round(np.random.randn(1000,200)[0][0]*1000))\n",
        "\n",
        "  model_summary = pd.DataFrame(columns =[])\n",
        "  for i in sorted(model_parameters_dict.keys()):\n",
        "    param_size = 0\n",
        "    if models != None:\n",
        "      param_size = get_n_params(models[i][0])\n",
        "      param_size += get_n_params(models[i][1])\n",
        "    row = pd.Series({\n",
        "                        'model_id':i,\n",
        "                        'encoder repeats' : model_parameters_dict[i][0]['repeats'],\n",
        "                        'decoder repeats' : model_parameters_dict[i][1]['repeats'],\n",
        "                        'parameters' :param_size\n",
        "    })\n",
        "    row_df = pd.DataFrame([row], index = [i])\n",
        "    model_summary = pd.concat([model_summary, row_df])\n",
        "  model_summary.to_csv(fileName)\n",
        "\n",
        "#Generate the list of repeat count parameters list\n",
        "\n",
        "#generate_model_parameters(generate_repeat_counts())\n",
        "\n",
        "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
        "    torch.save(state, filename)\n",
        "    if is_best:\n",
        "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
        "\n",
        "def save_dict(dictionary, filename):\n",
        "  np.save(filename, dictionary)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points, points2):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(points)\n",
        "    plt.plot(points2)\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmGEoDTZX3f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}